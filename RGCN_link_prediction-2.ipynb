{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RGCN_link_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgLIFk0LxRoo",
        "outputId": "ec301dc2-9246-433b-ad62-e8ac3aa4b78d"
      },
      "source": [
        "pip install rdflib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 20.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20kB 25.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 40kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 51kB 21.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 61kB 21.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 71kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 81kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 92kB 23.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 102kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 112kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 122kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 133kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 143kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 153kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 163kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 174kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 184kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 194kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 204kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 215kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 225kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 25.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rdflib) (1.15.0)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib) (2.4.7)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.0 rdflib-5.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Bir7lpxZ9S",
        "outputId": "907d5747-312e-4d9f-8bb7-9830310b9053"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "!pip install bert-serving-client\n",
        "!pip install -U bert-serving-server[http]\n",
        "\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip\n",
        "!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &\n",
        "\n",
        "\n",
        "!ls  # you should see uncased_something_.zip\n",
        "\n",
        "\n",
        "from bert_serving.client import BertClient\n",
        "bc = BertClient(check_length=False)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting bert-serving-client\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/09/aae1405378a848b2e87769ad89a43d6d71978c4e15534ca48e82e723a72f/bert_serving_client-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.7/dist-packages (from bert-serving-client) (22.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bert-serving-client) (1.19.5)\n",
            "Installing collected packages: bert-serving-client\n",
            "Successfully installed bert-serving-client-1.10.0\n",
            "Collecting bert-serving-server[http]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/bd/cab677bbd0c5fb08b72e468371d2bca6ed9507785739b4656b0b5470d90b/bert_serving_server-1.10.0-py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.15.0)\n",
            "Collecting GPUtil>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=17.1.0 in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (22.1.0)\n",
            "Collecting flask-cors; extra == \"http\"\n",
            "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
            "Collecting flask-compress; extra == \"http\"\n",
            "  Downloading https://files.pythonhosted.org/packages/75/fa/a3c96f3f367ad1d6532fa8394c9a6f5879513868207096f6b41f4168b342/Flask_Compress-1.10.1-py3-none-any.whl\n",
            "Collecting flask-json; extra == \"http\"\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/2d/4c21d98b11f3a206fabbdd965b53a2ca3ee9fab7646c93cf36c060e8f1a4/Flask_JSON-0.3.4-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: bert-serving-client; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: flask; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.1.4)\n",
            "Collecting brotli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ea/5bd575511b37bbd1c794606a0a621e6feff8e96b7dd007a86a5d218b2d94/Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 28.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask; extra == \"http\"->bert-serving-server[http]) (2.0.1)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=4751c012b0b0f8e3eb60f76c38c46b47d215c07d648ec605b215ae6b13f0b8c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil, flask-cors, brotli, flask-compress, flask-json, bert-serving-server\n",
            "Successfully installed GPUtil-1.4.0 bert-serving-server-1.10.0 brotli-1.0.9 flask-compress-1.10.1 flask-cors-3.0.10 flask-json-0.3.4\n",
            "--2021-06-21 07:27:43--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.62.128, 172.253.115.128, 172.253.122.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.62.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   200MB/s    in 1.9s    \n",
            "\n",
            "2021-06-21 07:27:45 (200 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
            "out.file     uncased_L-12_H-768_A-12\t  updated_results.ttl\n",
            "sample_data  uncased_L-12_H-768_A-12.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIruKzECcbpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209e5bd3-699a-4d2c-99cd-fb9d08c7e574"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6MB 23.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 31.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 225kB 25.4MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xWfHI6LcqHr"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import rdflib as rdf\n",
        "import numpy as np\n",
        "from torch_geometric.data import (InMemoryDataset, Data)\n",
        "from collections import Counter"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLJnibSZ0zax"
      },
      "source": [
        "import math\n",
        "\n",
        "def uniform(size, tensor):\n",
        "    if tensor is not None:\n",
        "        bound = 1.0 / math.sqrt(size)\n",
        "        tensor.data.uniform_(-bound, bound)\n",
        "\n",
        "\n",
        "def kaiming_uniform(tensor, fan, a):\n",
        "    if tensor is not None:\n",
        "        bound = math.sqrt(6 / ((1 + a**2) * fan))\n",
        "        tensor.data.uniform_(-bound, bound)\n",
        "\n",
        "\n",
        "def glorot(tensor):\n",
        "    if tensor is not None:\n",
        "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def glorot_orthogonal(tensor, scale):\n",
        "    if tensor is not None:\n",
        "        torch.nn.init.orthogonal_(tensor.data)\n",
        "        scale /= ((tensor.size(-2) + tensor.size(-1)) * tensor.var())\n",
        "        tensor.data *= scale.sqrt()\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)\n",
        "\n",
        "\n",
        "def ones(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(1)\n",
        "\n",
        "\n",
        "def normal(tensor, mean, std):\n",
        "    if tensor is not None:\n",
        "        tensor.data.normal_(mean, std)\n",
        "\n",
        "\n",
        "def reset(nn):\n",
        "    def _reset(item):\n",
        "        if hasattr(item, 'reset_parameters'):\n",
        "            item.reset_parameters()\n",
        "\n",
        "    if nn is not None:\n",
        "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
        "            for item in nn.children():\n",
        "                _reset(item)\n",
        "        else:\n",
        "            _reset(nn)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp6GzNJ81D2n"
      },
      "source": [
        "from copy import copy\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch_sparse import SparseTensor\n",
        "\n",
        "\n",
        "@torch.jit._overload\n",
        "def maybe_num_nodes(edge_index, num_nodes=None):\n",
        "    # type: (Tensor, Optional[int]) -> int\n",
        "    pass\n",
        "\n",
        "\n",
        "@torch.jit._overload\n",
        "def maybe_num_nodes(edge_index, num_nodes=None):\n",
        "    # type: (SparseTensor, Optional[int]) -> int\n",
        "    pass\n",
        "\n",
        "\n",
        "def maybe_num_nodes(edge_index, num_nodes=None):\n",
        "    if num_nodes is not None:\n",
        "        return num_nodes\n",
        "    elif isinstance(edge_index, Tensor):\n",
        "        return int(edge_index.max()) + 1\n",
        "    else:\n",
        "        return max(edge_index.size(0), edge_index.size(1))\n",
        "\n",
        "\n",
        "def maybe_num_nodes_dict(edge_index_dict, num_nodes_dict=None):\n",
        "    num_nodes_dict = {} if num_nodes_dict is None else copy(num_nodes_dict)\n",
        "\n",
        "    found_types = list(num_nodes_dict.keys())\n",
        "\n",
        "    for keys, edge_index in edge_index_dict.items():\n",
        "\n",
        "        key = keys[0]\n",
        "        if key not in found_types:\n",
        "            N = int(edge_index[0].max() + 1)\n",
        "            num_nodes_dict[key] = max(N, num_nodes_dict.get(key, N))\n",
        "\n",
        "        key = keys[-1]\n",
        "        if key not in found_types:\n",
        "            N = int(edge_index[1].max() + 1)\n",
        "            num_nodes_dict[key] = max(N, num_nodes_dict.get(key, N))\n",
        "\n",
        "    return num_nodes_dict\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkcOdkZC0cY8"
      },
      "source": [
        "import random\n",
        "from torch_geometric.utils import degree, to_undirected\n",
        "\n",
        "\n",
        "def sample(high: int, size: int, device=None):\n",
        "    size = min(high, size)\n",
        "    return torch.tensor(random.sample(range(high), size), device=device)\n",
        "\n",
        "\n",
        "def negative_sampling(edge_index, num_nodes=None, num_neg_samples=None,\n",
        "                      method=\"sparse\", force_undirected=False):\n",
        "    r\"\"\"Samples random negative edges of a graph given by :attr:`edge_index`.\n",
        "\n",
        "    Args:\n",
        "        edge_index (LongTensor): The edge indices.\n",
        "        num_nodes (int, optional): The number of nodes, *i.e.*\n",
        "            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)\n",
        "        num_neg_samples (int, optional): The (approximate) number of negative\n",
        "            samples to return. If set to :obj:`None`, will try to return a\n",
        "            negative edge for every positive edge. (default: :obj:`None`)\n",
        "        method (string, optional): The method to use for negative sampling,\n",
        "            *i.e.*, :obj:`\"sparse\"` or :obj:`\"dense\"`.\n",
        "            This is a memory/runtime trade-off.\n",
        "            :obj:`\"sparse\"` will work on any graph of any size, while\n",
        "            :obj:`\"dense\"` can perform faster true-negative checks.\n",
        "            (default: :obj:`\"sparse\"`)\n",
        "        force_undirected (bool, optional): If set to :obj:`True`, sampled\n",
        "            negative edges will be undirected. (default: :obj:`False`)\n",
        "\n",
        "    :rtype: LongTensor\n",
        "    \"\"\"\n",
        "\n",
        "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
        "    num_neg_samples = num_neg_samples or edge_index.size(1)\n",
        "\n",
        "    # Handle '|V|^2 - |E| < |E|'.\n",
        "    size = num_nodes * num_nodes\n",
        "    num_neg_samples = min(num_neg_samples, size - edge_index.size(1))\n",
        "\n",
        "    row, col = edge_index\n",
        "\n",
        "    if force_undirected:\n",
        "        num_neg_samples = num_neg_samples // 2\n",
        "\n",
        "        # Upper triangle indices: N + ... + 1 = N (N + 1) / 2\n",
        "        size = (num_nodes * (num_nodes + 1)) // 2\n",
        "\n",
        "        # Remove edges in the lower triangle matrix.\n",
        "        mask = row <= col\n",
        "        row, col = row[mask], col[mask]\n",
        "\n",
        "        # idx = N * i + j - i * (i+1) / 2\n",
        "        idx = row * num_nodes + col - row * (row + 1) // 2\n",
        "    else:\n",
        "        idx = row * num_nodes + col\n",
        "\n",
        "    # Percentage of edges to oversample so that we are save to only sample once\n",
        "    # (in most cases).\n",
        "    alpha = abs(1 / (1 - 1.1 * (edge_index.size(1) / size)))\n",
        "\n",
        "    if method == 'dense':\n",
        "        mask = edge_index.new_ones(size, dtype=torch.bool)\n",
        "        mask[idx] = False\n",
        "        mask = mask.view(-1)\n",
        "\n",
        "        perm = sample(size, int(alpha * num_neg_samples),\n",
        "                      device=edge_index.device)\n",
        "        perm = perm[mask[perm]][:num_neg_samples]\n",
        "\n",
        "    else:\n",
        "        perm = sample(size, int(alpha * num_neg_samples))\n",
        "        mask = torch.from_numpy(np.isin(perm, idx.to('cpu'))).to(torch.bool)\n",
        "        perm = perm[~mask][:num_neg_samples].to(edge_index.device)\n",
        "\n",
        "    if force_undirected:\n",
        "        # (-sqrt((2 * N + 1)^2 - 8 * perm) + 2 * N + 1) / 2\n",
        "        row = torch.floor((-torch.sqrt((2. * num_nodes + 1.)**2 - 8. * perm) +\n",
        "                           2 * num_nodes + 1) / 2)\n",
        "        col = perm - row * (2 * num_nodes - row - 1) // 2\n",
        "        neg_edge_index = torch.stack([row, col], dim=0).long()\n",
        "        neg_edge_index = to_undirected(neg_edge_index)\n",
        "    else:\n",
        "        row = perm // num_nodes\n",
        "        col = perm % num_nodes\n",
        "        neg_edge_index = torch.stack([row, col], dim=0).long()\n",
        "\n",
        "    return neg_edge_index\n",
        "\n",
        "\n",
        "def structured_negative_sampling(edge_index, num_nodes=None):\n",
        "    r\"\"\"Samples a negative edge :obj:`(i,k)` for every positive edge\n",
        "    :obj:`(i,j)` in the graph given by :attr:`edge_index`, and returns it as a\n",
        "    tuple of the form :obj:`(i,j,k)`.\n",
        "\n",
        "    Args:\n",
        "        edge_index (LongTensor): The edge indices.\n",
        "        num_nodes (int, optional): The number of nodes, *i.e.*\n",
        "            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)\n",
        "\n",
        "    :rtype: (LongTensor, LongTensor, LongTensor)\n",
        "    \"\"\"\n",
        "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
        "\n",
        "    i, j = edge_index.to('cpu')\n",
        "    idx_1 = i * num_nodes + j\n",
        "\n",
        "    k = torch.randint(num_nodes, (i.size(0), ), dtype=torch.long)\n",
        "    idx_2 = i * num_nodes + k\n",
        "\n",
        "    mask = torch.from_numpy(np.isin(idx_2, idx_1)).to(torch.bool)\n",
        "    rest = mask.nonzero(as_tuple=False).view(-1)\n",
        "    while rest.numel() > 0:  # pragma: no cover\n",
        "        tmp = torch.randint(num_nodes, (rest.numel(), ), dtype=torch.long)\n",
        "        idx_2 = i[rest] * num_nodes + tmp\n",
        "        mask = torch.from_numpy(np.isin(idx_2, idx_1)).to(torch.bool)\n",
        "        k[rest] = tmp\n",
        "        rest = rest[mask.nonzero(as_tuple=False).view(-1)]\n",
        "\n",
        "    return edge_index[0], edge_index[1], k.to(edge_index.device)\n",
        "\n",
        "\n",
        "def batched_negative_sampling(edge_index, batch, num_neg_samples=None,\n",
        "                              method=\"sparse\", force_undirected=False):\n",
        "    r\"\"\"Samples random negative edges of multiple graphs given by\n",
        "    :attr:`edge_index` and :attr:`batch`.\n",
        "\n",
        "    Args:\n",
        "        edge_index (LongTensor): The edge indices.\n",
        "        batch (LongTensor): Batch vector\n",
        "            :math:`\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N`, which assigns each\n",
        "            node to a specific example.\n",
        "        num_neg_samples (int, optional): The number of negative samples to\n",
        "            return. If set to :obj:`None`, will try to return a negative edge\n",
        "            for every positive edge. (default: :obj:`None`)\n",
        "        method (string, optional): The method to use for negative sampling,\n",
        "            *i.e.*, :obj:`\"sparse\"` or :obj:`\"dense\"`.\n",
        "            This is a memory/runtime trade-off.\n",
        "            :obj:`\"sparse\"` will work on any graph of any size, while\n",
        "            :obj:`\"dense\"` can perform faster true-negative checks.\n",
        "            (default: :obj:`\"sparse\"`)\n",
        "        force_undirected (bool, optional): If set to :obj:`True`, sampled\n",
        "            negative edges will be undirected. (default: :obj:`False`)\n",
        "\n",
        "    :rtype: LongTensor\n",
        "    \"\"\"\n",
        "    split = degree(batch[edge_index[0]], dtype=torch.long).tolist()\n",
        "    edge_indices = torch.split(edge_index, split, dim=1)\n",
        "    num_nodes = degree(batch, dtype=torch.long)\n",
        "    cum_nodes = torch.cat([batch.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]])\n",
        "\n",
        "    neg_edge_indices = []\n",
        "    for edge_index, N, C in zip(edge_indices, num_nodes.tolist(),\n",
        "                                cum_nodes.tolist()):\n",
        "        neg_edge_index = negative_sampling(edge_index - C, N, num_neg_samples,\n",
        "                                           method, force_undirected) + C\n",
        "        neg_edge_indices.append(neg_edge_index)\n",
        "\n",
        "    return torch.cat(neg_edge_indices, dim=1)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsSXQ90v1e3q"
      },
      "source": [
        "from typing import Optional, Union, Tuple\n",
        "from torch_geometric.typing import OptTensor, Adj\n",
        "\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter as Param\n",
        "from torch.nn import Parameter\n",
        "from torch_scatter import scatter\n",
        "from torch_sparse import SparseTensor, matmul, masked_select_nnz\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "import logging \n",
        "#logging.basicConfig(filename='RGCNConv.log', filemode='a', format='%(asctime)s %(message)s')\n",
        "\n",
        "@torch.jit._overload\n",
        "def masked_edge_index(edge_index, edge_mask):\n",
        "    # type: (Tensor, Tensor) -> Tensor\n",
        "    pass\n",
        "\n",
        "\n",
        "@torch.jit._overload\n",
        "def masked_edge_index(edge_index, edge_mask):\n",
        "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
        "    pass\n",
        "\n",
        "\n",
        "def masked_edge_index(edge_index, edge_mask):\n",
        "#    logging.warning(f'edge_index_shape is = {edge_index.shape}, edge_mask_shape is = {edge_mask.shape} and edge_index[:, edge_mask] is = {edge_index[:, edge_mask]}')\n",
        "    if isinstance(edge_index, Tensor):\n",
        "        #print(\"If edge_index is a Tensor \" + str(edge_index.type))\n",
        "        return edge_index[:, edge_mask]\n",
        "    else:\n",
        "        #print(\"Otherwise\")\n",
        "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
        "\n",
        "\n",
        "class RGCNConv(MessagePassing):\n",
        "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
        "    Relational Data with Graph Convolutional Networks\"\n",
        "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
        "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
        "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
        "\n",
        "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
        "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
        "    stores a relation identifier\n",
        "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
        "\n",
        "    .. note::\n",
        "        This implementation is as memory-efficient as possible by iterating\n",
        "        over each individual relation type.\n",
        "        Therefore, it may result in low GPU utilization in case the graph has a\n",
        "        large number of relations.\n",
        "        As an alternative approach, :class:`FastRGCNConv` does not iterate over\n",
        "        each individual type, but may consume a large amount of memory to\n",
        "        compensate.\n",
        "        We advise to check out both implementations to see which one fits your\n",
        "        needs.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int or tuple): Size of each input sample. A tuple\n",
        "            corresponds to the sizes of source and target dimensionalities.\n",
        "            In case no input features are given, this argument should\n",
        "            correspond to the number of nodes in your graph.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_relations (int): Number of relations.\n",
        "        num_bases (int, optional): If set to not :obj:`None`, this layer will\n",
        "            use the basis-decomposition regularization scheme where\n",
        "            :obj:`num_bases` denotes the number of bases to use.\n",
        "            (default: :obj:`None`)\n",
        "        num_blocks (int, optional): If set to not :obj:`None`, this layer will\n",
        "            use the block-diagonal-decomposition regularization scheme where\n",
        "            :obj:`num_blocks` denotes the number of blocks to use.\n",
        "            (default: :obj:`None`)\n",
        "        aggr (string, optional): The aggregation scheme to use\n",
        "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
        "            (default: :obj:`\"mean\"`)\n",
        "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
        "            not add transformed root node features to the output.\n",
        "            (default: :obj:`True`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
        "                 out_channels: int,\n",
        "                 num_relations: int,\n",
        "                 num_bases: Optional[int] = None,\n",
        "                 num_blocks: Optional[int] = None,\n",
        "                 aggr: str = 'mean',\n",
        "                 root_weight: bool = True,\n",
        "                 bias: bool = True, **kwargs):  # yapf: disable\n",
        "\n",
        "        super(RGCNConv, self).__init__(aggr=aggr, node_dim=0, **kwargs)\n",
        "\n",
        "        if num_bases is not None and num_blocks is not None:\n",
        "            raise ValueError('Can not apply both basis-decomposition and '\n",
        "                             'block-diagonal-decomposition at the same time.')\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_relations = num_relations\n",
        "        self.num_bases = num_bases\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "        self.in_channels_l = in_channels[0]\n",
        "\n",
        "        if num_bases is not None:\n",
        "            self.weight = Parameter(\n",
        "                torch.Tensor(num_bases, in_channels[0], out_channels))\n",
        "            self.comp = Parameter(torch.Tensor(num_relations, num_bases))\n",
        "\n",
        "        elif num_blocks is not None:\n",
        "            assert (in_channels[0] % num_blocks == 0\n",
        "                    and out_channels % num_blocks == 0)\n",
        "            self.weight = Parameter(\n",
        "                torch.Tensor(num_relations, num_blocks,\n",
        "                             in_channels[0] // num_blocks,\n",
        "                             out_channels // num_blocks))\n",
        "            self.register_parameter('comp', None)\n",
        "\n",
        "        else:\n",
        "            self.weight = Parameter(\n",
        "                torch.Tensor(num_relations, in_channels[0], out_channels))\n",
        "            self.register_parameter('comp', None)\n",
        "\n",
        "        if root_weight:\n",
        "            self.root = Param(torch.Tensor(in_channels[1], out_channels))\n",
        "        else:\n",
        "            self.register_parameter('root', None)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Param(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight)\n",
        "        glorot(self.comp)\n",
        "        glorot(self.root)\n",
        "        zeros(self.bias)\n",
        "\n",
        "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
        "                edge_index: Adj, edge_type: OptTensor = None):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            x: The input node features. Can be either a :obj:`[num_nodes,\n",
        "                in_channels]` node feature matrix, or an optional\n",
        "                one-dimensional node index tensor (in which case input features\n",
        "                are treated as trainable node embeddings).\n",
        "                Furthermore, :obj:`x` can be of type :obj:`tuple` denoting\n",
        "                source and destination node features.\n",
        "            edge_type: The one-dimensional relation type/index for each edge in\n",
        "                :obj:`edge_index`.\n",
        "                Should be only :obj:`None` in case :obj:`edge_index` is of type\n",
        "                :class:`torch_sparse.tensor.SparseTensor`.\n",
        "                (default: :obj:`None`)\n",
        "        \"\"\"\n",
        "        logging.warning(f'edge_index_in_forward_start is = {edge_index.shape}')\n",
        "        # Convert input features to a pair of node features or node indices.\n",
        "        #print(\"x has shape: \" + str(x.shape))       \n",
        "        x_l: OptTensor = None\n",
        "        if isinstance(x, tuple):\n",
        "            x_l = x[0]\n",
        "        else:\n",
        "            x_l = x\n",
        "        if x_l is None:\n",
        "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
        "\n",
        "        x_r: Tensor = x_l\n",
        "        if isinstance(x, tuple):\n",
        "            x_r = x[1]\n",
        "        #print(x_l)\n",
        "        size = (x_l.size(0), x_r.size(0))\n",
        "\n",
        "        if isinstance(edge_index, SparseTensor):\n",
        "            edge_type = edge_index.storage.value()  \n",
        "        assert edge_type is not None\n",
        "\n",
        "        # propagate_type: (x: Tensor)\n",
        "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
        "\n",
        "        weight = self.weight\n",
        "        if self.num_bases is not None:  # Basis-decomposition =================\n",
        "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
        "                self.num_relations, self.in_channels_l, self.out_channels)\n",
        "\n",
        "        if self.num_blocks is not None:  # Block-diagonal-decomposition =====\n",
        "\n",
        "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
        "                raise ValueError('Block-diagonal decomposition not supported '\n",
        "                                 'for non-continuous input features.')\n",
        "\n",
        "            for i in range(self.num_relations):\n",
        "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
        "                h = self.propagate(tmp, x=x_l, size=size)\n",
        "                h = h.view(-1, weight.size(1), weight.size(2))\n",
        "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
        "                out += h.contiguous().view(-1, self.out_channels)\n",
        "\n",
        "        else: # No regularization/Basis-decomposition ========================\n",
        "            for i in range(self.num_relations):\n",
        "                #print(masked_edge_index(edge_index, edge_type == i))\n",
        "                #logging.warning(f'edge_index is = {edge_index}, edge_type_dim = {(edge_type.shape)} ,edge_type_i is = {edge_type == i} and edge_type_i_dim = {(edge_type == i).shape})') \n",
        "                tmp = masked_edge_index(edge_index, edge_type == i) \n",
        "                #logging.warning(f'tmp is = {tmp},size is = {size}') \n",
        "                if x_l.dtype == torch.long:\n",
        "\n",
        "                   out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
        "                  \n",
        "                    #out += self.propagate(edge_index, x=weight[i, x_l], size=size)\n",
        "                else:\n",
        "                   \n",
        "                    #print(\"Size is:\" + str(size))\n",
        "                    #logging.warning(f'tmp is = {tmp.shape} and x_l is = {x_l.shape} and size is = {size}') \n",
        "                    h = self.propagate(tmp, x=x_l, size=size)\n",
        "                    #h = self.propagate(edge_index, x=x_l, size=size)\n",
        "                    out = out + (h @ weight[i])\n",
        "\n",
        "        root = self.root\n",
        "        if root is not None:\n",
        "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
        "\n",
        "        if self.bias is not None:\n",
        "            out += self.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j: Tensor) -> Tensor:\n",
        "        return x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        adj_t = adj_t.set_value(None, layout=None)\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, num_relations={})'.format(self.__class__.__name__,\n",
        "                                                     self.in_channels,\n",
        "                                                     self.out_channels,\n",
        "                                                     self.num_relations)\n",
        "\n",
        "\n",
        "class FastRGCNConv(RGCNConv):\n",
        "    r\"\"\"See :class:`RGCNConv`.\"\"\"\n",
        "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
        "                edge_index: Adj, edge_type: OptTensor = None):\n",
        "        \"\"\"\"\"\"\n",
        "        self.fuse = False\n",
        "        assert self.aggr in ['add', 'sum', 'mean']\n",
        "\n",
        "        # Convert input features to a pair of node features or node indices.\n",
        "        x_l: OptTensor = None\n",
        "        if isinstance(x, tuple):\n",
        "            x_l = x[0]\n",
        "        else:\n",
        "            x_l = x\n",
        "        if x_l is None:\n",
        "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
        "\n",
        "        x_r: Tensor = x_l\n",
        "        if isinstance(x, tuple):\n",
        "            x_r = x[1]\n",
        "\n",
        "        size = (x_l.size(0), x_r.size(0))\n",
        "\n",
        "        # propagate_type: (x: Tensor, edge_type: OptTensor)\n",
        "        out = self.propagate(edge_index, x=x_l, edge_type=edge_type, size=size)\n",
        "\n",
        "        root = self.root\n",
        "        if root is not None:\n",
        "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
        "\n",
        "        if self.bias is not None:\n",
        "            out += self.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_type: Tensor, index: Tensor) -> Tensor:\n",
        "        weight = self.weight\n",
        "        if self.num_bases is not None:  # Basis-decomposition =================\n",
        "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
        "                self.num_relations, self.in_channels_l, self.out_channels)\n",
        "\n",
        "        if self.num_blocks is not None:  # Block-diagonal-decomposition =======\n",
        "            if x_j.dtype == torch.long:\n",
        "                raise ValueError('Block-diagonal decomposition not supported '\n",
        "                                 'for non-continuous input features.')\n",
        "\n",
        "            weight = weight[edge_type].view(-1, weight.size(2), weight.size(3))\n",
        "            x_j = x_j.view(-1, 1, weight.size(1))\n",
        "            return torch.bmm(x_j, weight).view(-1, self.out_channels)\n",
        "\n",
        "        else:  # No regularization/Basis-decomposition ========================\n",
        "            if x_j.dtype == torch.long:\n",
        "                weight_index = edge_type * weight.size(1) + index\n",
        "                return weight.view(-1, self.out_channels)[weight_index]\n",
        "\n",
        "            return torch.bmm(x_j.unsqueeze(-2), weight[edge_type]).squeeze(-2)\n",
        "\n",
        "    def aggregate(self, inputs: Tensor, edge_type: Tensor, index: Tensor,\n",
        "                  dim_size: Optional[int] = None) -> Tensor:\n",
        "\n",
        "        # Compute normalization in separation for each `edge_type`.\n",
        "        if self.aggr == 'mean':\n",
        "            norm = F.one_hot(edge_type, self.num_relations).to(torch.float)\n",
        "            norm = scatter(norm, index, dim=0, dim_size=dim_size)[index]\n",
        "            norm = torch.gather(norm, 1, edge_type.view(-1, 1))\n",
        "            norm = 1. / norm.clamp_(1.)\n",
        "            inputs = norm * inputs\n",
        "\n",
        "        return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzWiGlr1sQh"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "\n",
        "def train_test_split_edges(data, val_ratio=0.05, test_ratio=0.1):\n",
        "    r\"\"\"Splits the edges of a :obj:`torch_geometric.data.Data` object\n",
        "    into positive and negative train/val/test edges, and adds attributes of\n",
        "    `train_pos_edge_index`, `train_neg_adj_mask`, `val_pos_edge_index`,\n",
        "    `val_neg_edge_index`, `test_pos_edge_index`, and `test_neg_edge_index`\n",
        "    to :attr:`data`.\n",
        "\n",
        "    Args:\n",
        "        data (Data): The data object.\n",
        "        val_ratio (float, optional): The ratio of positive validation\n",
        "            edges. (default: :obj:`0.05`)\n",
        "        test_ratio (float, optional): The ratio of positive test\n",
        "            edges. (default: :obj:`0.1`)\n",
        "\n",
        "    :rtype: :class:`torch_geometric.data.Data`\n",
        "    \"\"\"\n",
        "\n",
        "    assert 'batch' not in data  # No batch-mode.\n",
        "    edge_list = data.edge_list\n",
        "    num_nodes = data.num_nodes\n",
        "    row, col = data.edge_index\n",
        "    data.edge_index = None\n",
        "\n",
        "    # Return upper triangular portion.\n",
        "    mask = row < col\n",
        "    row, col = row[mask], col[mask]\n",
        "\n",
        "    n_v = int(math.floor(val_ratio * row.size(0)))\n",
        "    n_t = int(math.floor(test_ratio * row.size(0)))\n",
        "\n",
        "    # Positive edges.\n",
        "    perm = torch.randperm(row.size(0))\n",
        "    row, col = row[perm], col[perm]\n",
        "\n",
        "    r, c = row[:n_v], col[:n_v]\n",
        "    data.val_pos_edge_index = torch.stack([r, c], dim=0)\n",
        "    r, c = row[n_v:n_v + n_t], col[n_v:n_v + n_t]\n",
        "    data.test_pos_edge_index = torch.stack([r, c], dim=0)\n",
        "\n",
        "    data.train_pos_edge_index_edge_type = []\n",
        "    r, c = row[n_v + n_t:], col[n_v + n_t:]\n",
        "    #print()\n",
        "    data.train_pos_edge_index = torch.stack([r, c], dim=0)\n",
        "    data.train_pos_edge_index = to_undirected(data.train_pos_edge_index)\n",
        "  #  print(edge_list[:10])\n",
        "    print(edge_list[0])\n",
        "    #print(edge_list[1])\n",
        "   # print(edge_list[2])\n",
        "    printtrain_pos_edge_index[0]()\n",
        "    for sub,rel,obj in zip(edge_list[0], edge_list[1], edge_list[2]):\n",
        "       index = None\n",
        "       # print( data.train_pos_edge_index[0].tolist().index(sub))\n",
        "       if sub in data.train_pos_edge_index[0]:\n",
        "        index =  data.train_pos_edge_index[0].tolist().index(sub) \n",
        "       print(\"Index is\" + str(index))\n",
        "       #print(data.train_pos_edge_index[1][index])\n",
        "       #print(data.train_pos_edge_index[0][index])\n",
        "\n",
        "       if index is not None:\n",
        "          print(\"Index is not None\")\n",
        "          if obj == data.train_pos_edge_index[1].tolist()[index]:\n",
        "            print(\"obj is \" + str(obj) + \"sub is: \" + str(sub) +  \"relation is \" + str(rel))\n",
        "            print(\"Obj and subj match found\")\n",
        "            print(data.train_pos_edge_index[1][index])\n",
        "            data.train_pos_edge_index_edge_type.append(rel)\n",
        "    \n",
        "    data.train_pos_edge_index_edge_type =  torch.tensor(data.train_pos_edge_index_edge_type, dtype=torch.long).t().contiguous()\n",
        "    \n",
        "    # Negative edges.\n",
        "    neg_adj_mask = torch.ones(num_nodes, num_nodes, dtype=torch.uint8)\n",
        "    neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)\n",
        "    neg_adj_mask[row, col] = 0\n",
        "\n",
        "    neg_row, neg_col = neg_adj_mask.nonzero(as_tuple=False).t()\n",
        "    perm = torch.randperm(neg_row.size(0))[:n_v + n_t]\n",
        "    neg_row, neg_col = neg_row[perm], neg_col[perm]\n",
        "\n",
        "    neg_adj_mask[neg_row, neg_col] = 0\n",
        "    data.train_neg_adj_mask = neg_adj_mask\n",
        "\n",
        "    row, col = neg_row[:n_v], neg_col[:n_v]\n",
        "    data.val_neg_edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "    row, col = neg_row[n_v:n_v + n_t], neg_col[n_v:n_v + n_t]\n",
        "    data.test_neg_edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "    return data"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAd-9rqUlBJA"
      },
      "source": [
        "from torch_geometric.data import (InMemoryDataset, Data)\n",
        "from collections import Counter\n",
        "import numpy \n",
        "import torch.nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "fc = torch.nn.Linear(768, 10)\n",
        "\n",
        "class EntitiesIOSPress(InMemoryDataset):    \n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EntitiesIOSPress, self)\n",
        "        self.data = None\n",
        "\n",
        "    @property\n",
        "    def num_relations(self):\n",
        "        return self.data.edge_type.max().item() + 1\n",
        "\n",
        "    @property\n",
        "    def num_features(self):\n",
        "        return self.data.tmp_num_features\n",
        "\n",
        "\n",
        "    #def num_classes(self):\n",
        "        #return self.data.train_y.max().item() + 1\n",
        "\n",
        "    def process(self):\n",
        "        g = rdf.Graph()\n",
        "        graph = g.parse(location='/content/updated_results.ttl', format='ttl')\n",
        "        freq_ = Counter(graph.predicates())\n",
        "\n",
        "        def freq(rel):\n",
        "          return freq_[rel] if rel in freq_ else 0\n",
        "\n",
        "        relations = sorted(set(g.predicates()), key=lambda rel: -freq(rel)) \n",
        "        nodes = {}\n",
        "        #returns the  authors\n",
        "        for e1,r,e2 in graph.triples((None, rdf.term.URIRef('https://schema.org/fullName'), None)):\n",
        "          author_embeddings = torch.from_numpy(bc.encode([e2]))\n",
        "          nodes[e1] = author_embeddings\n",
        "        for e1,r,e2 in graph.triples((None, rdf.term.URIRef('https://schema.org/coAuthorOf'), None)):\n",
        "          if graph.value(subject = e2, predicate = rdf.term.URIRef('https://schema.org/fullName')) is not None:\n",
        "            author_embeddings = torch.from_numpy(bc.encode([graph.value(subject = e2, predicate = rdf.term.URIRef('https://schema.org/fullName'))]))\n",
        "            nodes[e2] = author_embeddings\n",
        "          else:\n",
        "            graph.remove((e1,r,e2))\n",
        "        #returns the articles\n",
        "        for e1,r,e2 in graph.triples((None, rdf.term.URIRef('https://schema.org/hasPubAbstract'), None)):\n",
        "          article_attribute_embeddings = []\n",
        "          abstract = e2\n",
        "          title = graph.value(subject = e1, predicate = rdf.term.URIRef('https://schema.org/hasPubTitle'))\n",
        "          keywords = graph.value(subject = e1, predicate = rdf.term.URIRef('https://schema.org/hasKeywords'))\n",
        "          if abstract is not None:\n",
        "            abstract_embedding = bc.encode([abstract])\n",
        "            article_attribute_embedding = abstract_embedding\n",
        "            #print(abstract_embedding.shape)\n",
        "            #print(abstract_embedding.dtype)\n",
        "          if title is not None: \n",
        "            title_embedding = bc.encode([title])\n",
        "           # print(title_embedding.shape)\n",
        "           # print(title_embedding.dtype)\n",
        "            if len(article_attribute_embedding) > 0:\n",
        "             # print(\"1st concat: torch.cat() start for article attribute\")\n",
        "              article_attribute_embedding = torch.cat([torch.from_numpy(article_attribute_embedding), torch.from_numpy(title_embedding)])\n",
        "             # print(\"torch.cat() finished for article attribute\")\n",
        "            else : article_attribute_embedding = torch.from_numpy(title_embedding)\n",
        "          if keywords is not None: \n",
        "            #print(keywords)\n",
        "           # print(\"Start torch.sum()\")\n",
        "            keywords_embedding = torch.cat([torch.from_numpy(bc.encode([kw])) for kw in keywords if kw is not ' '])\n",
        "           # print(\"Finished torch.sum()\")\n",
        "            if len(article_attribute_embedding) > 0:\n",
        "            #  print(\"Start 2nd concat\")\n",
        "              article_attribute_embedding = torch.cat([article_attribute_embedding, keywords_embedding])\n",
        "            #  print(\"Done torch.cat()\")\n",
        "            else : article_attribute_embedding = torch.from_numpy(keywords_embedding)\n",
        "         # print(\"Done with all torch.cat()\")\n",
        "          #pass through dense layer: TODO\n",
        "          #article_attribute_embedding = torch.cat([bc.encode([abstract]), bc.encode([title]), torch.sum([bc.encode([keywords]) for kw in ])])#pass through dense layer\n",
        "          nodes[e1] = article_attribute_embedding\n",
        "        #returns the affiliations\n",
        "        for e1,r,e2 in graph.triples((None, rdf.term.URIRef('https://schema.org/hasName'), None)):\n",
        "          affiliation_embedding = []\n",
        "          affiliation_embedding =  torch.from_numpy(bc.encode([e2]))\n",
        "          nodes[e1] = affiliation_embedding\n",
        "        #print(article_attribute_embedding.shape)\n",
        "        nodes_dict = {key: nodeID for nodeID, key in enumerate(nodes)}\n",
        "        relations_dict = {rel: i for i, rel in enumerate(list(relations))}\n",
        "       # print(list(nodes_dict.items())[:5])\n",
        "       # print(relations_dict)\n",
        "        embeddings = []\n",
        "        edge_list = []\n",
        "        labels = {}\n",
        "        labels_set = set()\n",
        "        for e1,r,e2 in graph.triples((None, None, None)):\n",
        "          if r == rdf.term.URIRef('https://schema.org/hasName'):\n",
        "              #print((e1,r,e2) + str(\"Got it\"))\n",
        "              graph.remove((e1,r,e2))\n",
        "          if r == rdf.term.URIRef('https://schema.org/hasPubTitle'):\n",
        "              #print((e1,r,e2) + str(\"Got it\"))\n",
        "              graph.remove((e1,r,e2))\n",
        "          if r == rdf.term.URIRef('https://schema.org/hasKeywords'):\n",
        "              #print(str(e1,r,e2) + str(\"Got it\"))\n",
        "              graph.remove((e1,r,e2))\n",
        "          if r == rdf.term.URIRef('https://schema.org/fullName'):\n",
        "              #print((e1,r,e2) + str(\"Got it\"))\n",
        "              graph.remove((e1,r,e2))\n",
        "          if r == rdf.term.URIRef('https://schema.org/hasPubAbstract'):\n",
        "              #print((e1,r,e2) + str(\"Got it\"))\n",
        "              graph.remove((e1,r,e2)) \n",
        "          if r == rdf.term.URIRef('https://schema.org/hasAffiliation'):\n",
        "              #print((e1,r,e2) + str(\"Got it\"))\n",
        "              graph.remove((e1,r,e2))\n",
        "        for e1,r,e2 in graph.triples((None, None, None)):\n",
        "          if (e1 and r and e2) is not None:\n",
        "                #print((e1,r,e2))\n",
        "                src, dst,rel = nodes_dict[e1], nodes_dict[e2], relations_dict[r]\n",
        "                edge_list.append([src, dst, 2 * rel])\n",
        "                edge_list.append([dst, src, 2 * rel + 1])\n",
        "               # print(nodes[e1].size())\n",
        "                #input_e1 = nodes[e1].view(batch_size, -1)\n",
        "                #input_e2 = nodes[e2].view(batch_size, -1)\n",
        "                #print(input_e1)\n",
        "                output_e1 = fc(nodes[e1])\n",
        "                output_e2 = fc(nodes[e2])\n",
        "                #print(output_e1.dtype)\n",
        "                #print(output_e1.size()) \n",
        "                embeddings.append([output_e1 ,output_e2]) #first tuple is the source and the second is dst as tensors.\n",
        "                embeddings.append([output_e2 ,output_e1])\n",
        "                #labels[src] = relations_dict[r]\n",
        "                #labels_set.add(labels[src])\n",
        "        #\n",
        "        # Simulate a 28 x 28 pixel, grayscale \"image\"\n",
        "       # input = torch.randn(1, 28, 28)\n",
        "        # Use view() to get [batch_size, num_features].\n",
        "        # -1 calculates the missing value given the other dim.\n",
        "       # input = input.view(batch_size, -1) # torch.Size([1, 784])\n",
        "        # Intialize the linear layer.\n",
        "       #\n",
        "        # Pass in the simulated image to the layer.\n",
        "        #output = fc(embeddings)\n",
        "       # print(output.shape)\n",
        "     #   labels_dict = {lab: i for i, lab in enumerate(list(labels_set))}\n",
        "     #   train_labels, tpytorch geometric\n",
        "       #  est_labels = list(labels_dict.values())[:80,:], list(labels_dict.values())[80:,:]\n",
        "       # train_y = torch.tensor(train_labels, dtype=torch.long)\n",
        "        # IF for loop for edge_list edge_list = sorted(edge_list, key=lambda x: (x[0], x[1], x[2])) # deterministic \n",
        "        #TO ADD: for loop for edge_list\n",
        "        edge = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "        embeddings = torch.tensor(embeddings, dtype=torch.long).t().contiguous()\n",
        "        edge_index, edge_type = edge[:2], edge[2]\n",
        "      #  embeddings = output\n",
        "        #print(embeddings[:3])\n",
        "        #print(edge_index.shape)\n",
        "        #print(edge_index[:3])\n",
        "\n",
        "        data = Data(edge_index=edge_index)\n",
        "        data.embeddings = embeddings\n",
        "        data.edge_list = edge     \n",
        "        data.edge_index = edge_index\n",
        "        data.edge_type = edge_type \n",
        "        #data.train_y = train_y\n",
        "        data.num_relations = edge_type.max().item() + 1\n",
        "        data.num_classes = 3\n",
        "        data.num_nodes = data.edge_index.max().item() + 1\n",
        "        self.data = data"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXkKcr9zraNW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "53716133-6ec3-4b6b-e0ff-e91211cad3e3"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "#logging.basicConfig(filename='Link_pred_simone.log', filemode='w', format='%(asctime)s %(message)s')\n",
        "dataset = EntitiesIOSPress()\n",
        "dataset.train_mask = dataset.val_mask = dataset.test_mask = dataset.y = None\n",
        "dataset.process()\n",
        "print('done processing')\n",
        "data = train_test_split_edges(dataset.data)\n",
        "print('done edges')\n",
        "print(data)\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = RGCNConv(data.num_nodes, 16, data.num_relations,\n",
        "                                num_bases=5)\n",
        "        self.conv2 = RGCNConv(16, data.num_classes, data.num_relations,\n",
        "                              num_bases=5)\n",
        "\n",
        "    def encode(self, edge_index, edge_type):\n",
        "        #print(\"Edge_index dim: \" + str(data.edge_index)) returns None???\n",
        "        #logging.warning(f'self.conv1(data.x = {data.x}, data.train_pos_edge_index_shape = {data.train_pos_edge_index.shape}, edge_type_shape = {(edge_type.shape)})')\n",
        "        x = self.conv1(None, edge_index, edge_type)\n",
        "        #logging.warning(f'self.conv1(data.train_pos_edge_index_shape = {data.edge_index.shape}, edge_type_shape = {(edge_type.shape)}) and x.relu() = {x.relu()}')\n",
        "        x.relu() \n",
        "        return self.conv2(x, edge_index, edge_type)\n",
        "\n",
        "\n",
        "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
        "         edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
        "         return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    def decode_all(self, z):\n",
        "        prob_adj = z @ z.t()\n",
        "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)\n",
        "\n",
        "def get_link_labels(pos_edge_index, neg_edge_index):\n",
        "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
        "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
        "    link_labels[:pos_edge_index.size(1)] = 1.\n",
        "    return link_labels\n",
        "\n",
        "\n",
        "def train(data):\n",
        "    model.train()\n",
        "\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=data.train_pos_edge_index, num_nodes=data.num_nodes,\n",
        "        num_neg_samples=data.train_pos_edge_index.size(1))\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #logging.warning(f'edge_type_shape_in_training = {(data.edge_type.shape)})')\n",
        "    z = model.encode(data.train_pos_edge_index, data.train_pos_edge_index_edge_type)\n",
        "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index)\n",
        "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
        "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
        "    link_probs = link_logits.sigmoid()\n",
        "    #print(\"Training roc auc =\" + str(roc_auc_score(link_labels.cpu(), link_probs.cpu())))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #print(\"Loss is:\" + str(loss))\n",
        "    return loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    model.eval()\n",
        "\n",
        "    z = model.encode(data.train_pos_edge_index, data.train_pos_edge_index_edge_type)\n",
        "\n",
        "    results = []\n",
        "    for prefix in [\"val\", \"test\"]:\n",
        "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
        "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
        "        link_logits = model.decode(z, pos_edge_index, neg_edge_index)\n",
        "        #print(str(link_logits) + \"this is link_logits\")\n",
        "        link_probs = link_logits.sigmoid()\n",
        "        link_labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
        "        results.append(roc_auc_score(link_labels.cpu(), link_probs.cpu()))\n",
        "        #print(\"results is: \" + str(results))\n",
        "    return results\n",
        "\n",
        "best_val_auc = test_auc = 0\n",
        "for epoch in range(1, 101):\n",
        "    #print('Start Training')\n",
        "    train_loss = train(data)\n",
        "    #print('Finish Training')\n",
        "    #print(\"Start Test Phase\")\n",
        "    val_auc, tmp_test_auc = test(data)\n",
        "    #print(\"Finish Test Phase\")\n",
        "    #print(\"val_auc: \" + str(val_auc) + \"> \" + str(best_val_auc) + \"best_value_auc\")\n",
        "    if val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        test_auc= tmp_test_auc\n",
        "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
        "    #print(\"Epoch \" + str(epoch))\n",
        "    print(log.format(epoch, train_loss, best_val_auc, test_auc))\n",
        "\n",
        "z = model.encode(data.train_pos_edge_index,data.train_pos_edge_index_edge_type)\n",
        "final_edge_index = model.decode_all(z)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-62599712e99a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEntitiesIOSPress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done processing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-57e677871a6d>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/updated_results.ttl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ttl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mfreq_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/graph.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, publicID, format, location, file, data, **args)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, graph, encoding, turtle)\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSinkParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseURI\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseURI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturtle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mturtle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1886\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetByteStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bindings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mloadStream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloadStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadBuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Not ideal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloadBuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mloadBuf\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# self._formula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, octets)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectiveOrStatement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0;31m#print(\"# next char: %s\" % s[j])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mdirectiveOrStatement\u001b[0;34m(self, argstr, h)\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mstatement\u001b[0;34m(self, argstr, i)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperty_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mproperty_list\u001b[0;34m(self, argstr, i, subj)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m  \u001b[0;31m# void but valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mverb\u001b[0;34m(self, argstr, i, res)\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'->'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mprop\u001b[0;34m(self, argstr, i, res)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mitem\u001b[0;34m(self, argstr, i, res)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mblankNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mpath\u001b[0;34m(self, argstr, i, res)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \"\"\"Parse the path production.\n\u001b[1;32m    844\u001b[0m         \"\"\"\n\u001b[0;32m--> 845\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodeOrLiteral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mj\u001b[0m   \u001b[0;31m# nope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mnodeOrLiteral\u001b[0;34m(self, argstr, i, res)\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnodeOrLiteral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0mstartline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m  \u001b[0;31m# Remember where for error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mnode\u001b[0;34m(self, argstr, i, res, subjectAlready)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# If this can be a named node, then check for a name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_ref2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36muri_ref2\u001b[0;34m(self, argstr, i, res)\u001b[0m\n\u001b[1;32m   1193\u001b[0m                         self.BadSyntax(argstr, i,\n\u001b[1;32m   1194\u001b[0m                                        \"Prefix \\\"%s:\\\" not bound\" % (pfx))\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0msymb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewSymbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msymb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/plugins/parsers/notation3.py\u001b[0m in \u001b[0;36mnewSymbol\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnewSymbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1720\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mURIRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnewBlankNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rdflib/term.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value, base)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}