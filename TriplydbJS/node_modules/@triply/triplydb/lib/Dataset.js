"use strict";
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    Object.defineProperty(o, k2, { enumerable: true, get: function() { return m[k]; } });
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.JobUpload = void 0;
const fs = __importStar(require("fs-extra"));
const utils_1 = require("./utils");
const debug_1 = __importDefault(require("debug"));
const Service_1 = __importDefault(require("./Service"));
const log = debug_1.default("triply:triplydb-js:upload");
const tus = require("@triply/tus-js-client");
const md5_1 = __importDefault(require("md5"));
const os_1 = require("os");
const path_1 = __importDefault(require("path"));
const zlib = __importStar(require("zlib"));
const n3 = __importStar(require("n3"));
const pumpify_1 = __importDefault(require("pumpify"));
const lodash_1 = require("lodash");
const Error_1 = require("./utils/Error");
const RequestHandler_1 = require("./RequestHandler");
const AsyncIteratorHelper_1 = __importDefault(require("./utils/AsyncIteratorHelper"));
const Asset_1 = __importDefault(require("./Asset"));
const Graph_1 = __importStar(require("./Graph"));
const query_string_1 = require("query-string");
const cross_fetch_1 = __importDefault(require("cross-fetch"));
class Dataset {
    constructor(app, owner, datasetName, datasetInfo) {
        this._app = app;
        this._name = datasetName;
        this._owner = owner;
        this._info = datasetInfo;
    }
    getServices() {
        return new AsyncIteratorHelper_1.default({
            error: Error_1.getErr(`Failed to get services`),
            getErrorMessage: async () => `Failed to get services for dataset ${await this._getDatasetNameWithOwner()}.`,
            app: this._app,
            getUrl: async () => this._app["_config"].url + (await this._getDatasetPath("/services")),
            mapResult: async (info) => {
                var _a;
                return new Service_1.default({
                    app: this._app,
                    datasetPath: await this._getDatasetPath(),
                    datasetNameWithOwner: await this._getDatasetNameWithOwner(),
                    name: info.name,
                    type: info.type,
                    reasoner: (_a = info.config) === null || _a === void 0 ? void 0 : _a.reasonerType,
                });
            },
        });
    }
    removeAllGraphs() {
        return this.clear("graphs");
    }
    async clear(resourceType, ...rest) {
        await Promise.all(lodash_1.uniq(rest.concat(resourceType)).map(async (typeToClear) => {
            var e_1, _a, e_2, _b;
            if (typeToClear === "graphs") {
                return RequestHandler_1._delete({
                    errorWithCleanerStack: Error_1.getErr(`Failed to remove all graphs for ${await this._getDatasetNameWithOwner()}.`),
                    app: this._app,
                    path: await this._getDatasetPath("/graphs"),
                    expectedResponseBody: "empty",
                });
            }
            else if (typeToClear === "assets") {
                try {
                    for (var _c = __asyncValues(this.getAssets()), _d; _d = await _c.next(), !_d.done;) {
                        let asset = _d.value;
                        await (asset === null || asset === void 0 ? void 0 : asset.delete());
                    }
                }
                catch (e_1_1) { e_1 = { error: e_1_1 }; }
                finally {
                    try {
                        if (_d && !_d.done && (_a = _c.return)) await _a.call(_c);
                    }
                    finally { if (e_1) throw e_1.error; }
                }
            }
            else if (typeToClear === "services") {
                try {
                    for (var _e = __asyncValues(this.getServices()), _f; _f = await _e.next(), !_f.done;) {
                        let service = _f.value;
                        await (service === null || service === void 0 ? void 0 : service.delete());
                    }
                }
                catch (e_2_1) { e_2 = { error: e_2_1 }; }
                finally {
                    try {
                        if (_f && !_f.done && (_b = _e.return)) await _b.call(_e);
                    }
                    finally { if (e_2) throw e_2.error; }
                }
            }
            else {
                throw Error_1.getErr(`Unrecognized resource type: ${typeToClear}`);
            }
        }));
        await this.getInfo(true);
        return this;
    }
    async exists() {
        try {
            await this.getInfo();
            return true;
        }
        catch (e) {
            if (e.statusCode === 404)
                return false;
            throw e;
        }
    }
    async getGraph(graphNameOrIri) {
        var e_3, _a;
        const graphName = typeof graphNameOrIri === "string" ? graphNameOrIri : graphNameOrIri.value;
        try {
            for (var _b = __asyncValues(this.getGraphs()), _c; _c = await _b.next(), !_c.done;) {
                let graph = _c.value;
                if (!graph)
                    break;
                if ((await graph.getInfo()).graphName === graphName)
                    return graph;
            }
        }
        catch (e_3_1) { e_3 = { error: e_3_1 }; }
        finally {
            try {
                if (_c && !_c.done && (_a = _b.return)) await _a.call(_b);
            }
            finally { if (e_3) throw e_3.error; }
        }
        throw Error_1.getErr(`Graph '${graphName}' not found in dataset ${await this._getDatasetNameWithOwner()}`);
    }
    async deleteGraph(graphNameOrIri) {
        const graphName = typeof graphNameOrIri === "string" ? graphNameOrIri : graphNameOrIri.value;
        const graph = await this.getGraph(graphName);
        await graph.delete();
    }
    _setInfo(info) {
        this._info = info;
        this._name = info.name;
        return this;
    }
    async _getDatasetPath(additionalPath) {
        const ownerName = await this._owner.getName();
        return "/datasets/" + ownerName + "/" + this._name + (additionalPath || "");
    }
    async _getDatasetNameWithOwner() {
        const ownerName = await this._owner.getName();
        return `${ownerName}/${this._name}`;
    }
    async getInfo(refresh = false) {
        if (!refresh && this._info)
            return this._info;
        const info = await RequestHandler_1._get({
            errorWithCleanerStack: Error_1.getErr(`Failed to get dataset information for ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath(),
        });
        this._setInfo(info);
        return info;
    }
    async getAsset(assetName, versionNumber) {
        return new Asset_1.default(this, (await RequestHandler_1._get({
            errorWithCleanerStack: Error_1.getErr(`Failed to get asset ${assetName} from dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath("/assets"),
            query: { fileName: assetName },
        })), versionNumber);
    }
    getAssets() {
        return new AsyncIteratorHelper_1.default({
            error: Error_1.getErr(`Failed to get assets`),
            getErrorMessage: async () => `Failed to get assets of dataset ${await this._getDatasetNameWithOwner()}.`,
            app: this._app,
            getUrl: async () => this._app["_config"].url + (await this._getDatasetPath("/assets")),
            mapResult: async (assetInfo) => new Asset_1.default(this, assetInfo),
        });
    }
    getGraphs() {
        return new AsyncIteratorHelper_1.default({
            error: Error_1.getErr(`Failed to get graphs`),
            getErrorMessage: async () => `Failed to get graphs of dataset ${await this._getDatasetNameWithOwner()}.`,
            app: this._app,
            getUrl: async () => this._app["_config"].url + (await this._getDatasetPath("/graphs")),
            mapResult: async (info) => new Graph_1.default(this, info),
        });
    }
    async _getDownloadPath(extension) {
        const dsPath = `${await this._getDatasetPath()}`;
        return `${dsPath}/download${extension || ""}`;
    }
    async graphsToFile(destinationPath, opts) {
        const parsedPath = path_1.default.parse(destinationPath);
        if (Graph_1.SUPPORTED_EXTENSIONS.findIndex((e) => parsedPath.base.endsWith(e)) === -1) {
            throw Error_1.getErr(`Failed so save graph as \`${parsedPath.base}\`. Supported extensions: [ ${Graph_1.SUPPORTED_EXTENSIONS.join(", ")} ]`);
        }
        if (!(await fs.pathExists(path_1.default.resolve(parsedPath.dir)))) {
            throw Error_1.getErr(`Directory doesn't exist: ${parsedPath.dir}`);
        }
        let extension = parsedPath.ext;
        let storeCompressed;
        if (typeof (opts === null || opts === void 0 ? void 0 : opts.compressed) === "boolean") {
            storeCompressed = opts.compressed;
        }
        else {
            storeCompressed = extension === ".gz";
        }
        if (extension === ".gz") {
            extension = path_1.default.extname(parsedPath.name);
        }
        const downloadUrlPath = await this._getDownloadPath(extension);
        const res = await cross_fetch_1.default(RequestHandler_1.getUrl({ app: this._app, errorWithCleanerStack: Error_1.getErr("Failed to download graphs"), path: downloadUrlPath }), { method: "get", compress: false, headers: { authorization: `bearer ${this._app["_config"].token}` } });
        const stream = new pumpify_1.default(res.body, ...(storeCompressed ? [] : [zlib.createGunzip()]), fs.createWriteStream(destinationPath));
        await new Promise((resolve, reject) => {
            stream.on("error", reject);
            stream.on("finish", resolve);
        });
    }
    async graphsToStream(type) {
        const stream = await RequestHandler_1.handleFetchAsStream("GET", {
            app: this._app,
            path: await this._getDownloadPath(".trig.gz"),
            errorWithCleanerStack: Error_1.getErr(`Failed to download graphs of dataset ${await this._getDatasetNameWithOwner()}.`),
        });
        if (type === "compressed") {
            return stream;
        }
        else {
            return new pumpify_1.default.obj(stream, zlib.createGunzip(), new n3.StreamParser());
        }
    }
    async graphsToStore() {
        const store = new n3.Store();
        const stream = await this.graphsToStream("rdf-js");
        await new Promise((resolve, reject) => {
            store.import(stream).on("finish", resolve).on("error", reject);
        });
        return store;
    }
    async importFromDataset(arg1, graphs_deprecated, overwrite_deprecated) {
        let { fromDataset, graphs, overwrite } = "fromDataset" in arg1
            ? arg1
            : {
                fromDataset: arg1,
                graphs: graphs_deprecated,
                overwrite: !!overwrite_deprecated,
            };
        if (overwrite && !(await this._app.isCompatible("2.2.7"))) {
            throw new Error_1.IncompatibleError("Overwriting graphs is only supported by TriplyDB API version 2.2.7 or greater");
        }
        if (!graphs) {
            const graphNames = await Promise.all((await fromDataset.getGraphs().toArray()).map((g) => g.getInfo().then((g) => g.graphName)));
            graphs = lodash_1.zipObject(graphNames, graphNames);
        }
        const fromDsInfo = await fromDataset.getInfo();
        const graphsToImport = [];
        for (const fromGraph in graphs) {
            graphsToImport.push({ from: fromGraph, to: graphs[fromGraph], overwrite: !!overwrite });
        }
        return RequestHandler_1._patch({
            errorWithCleanerStack: Error_1.getErr(`Tried importing from dataset ${await fromDataset._getDatasetNameWithOwner()}. Failed to write the changes to ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath("/imports"),
            data: [
                {
                    dataset: {
                        ownerName: fromDsInfo.owner.accountName,
                        datasetName: fromDsInfo.name,
                    },
                    graphs: graphsToImport,
                },
            ],
        });
    }
    async update(config) {
        this._setInfo(await RequestHandler_1._patch({
            errorWithCleanerStack: Error_1.getErr(`Failed to update dataset information of ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath(),
            data: config,
        }));
        return this;
    }
    async copy(toAccountName, newDatasetName) {
        const newDs = await RequestHandler_1._post({
            errorWithCleanerStack: Error_1.getErr(`Failed to copy dataset ${await this._getDatasetNameWithOwner()} to ${toAccountName}.`),
            app: this._app,
            path: await this._getDatasetPath("/copy"),
            data: { toAccount: toAccountName, name: newDatasetName },
        });
        const toAccount = await this._app.getAccount(toAccountName);
        return toAccount.getDataset(newDs.name)._setInfo(newDs);
    }
    async renameGraph(from, to) {
        const graph = await this.getGraph(from);
        await graph.rename(to);
        return graph;
    }
    async delete() {
        await RequestHandler_1._delete({
            errorWithCleanerStack: Error_1.getErr(`Failed to delete dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath(),
            expectedResponseBody: "empty",
        });
        this._info = undefined;
        this.lastJob = undefined;
    }
    async setAvatar(pathBufferOrFile) {
        const info = await this.getInfo();
        await RequestHandler_1._post({
            errorWithCleanerStack: Error_1.getErr(`Failed to set avatar of dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: "/imgs/avatars/d/" + info.id,
            attach: { avatar: pathBufferOrFile },
        });
        await this.getInfo(true);
        return this;
    }
    _throwIfJobRunning(dsId) {
        if (datasetsWithOngoingJob[dsId]) {
            throw Error_1.getErr("There is already an ongoing job for this dataset. Await that one first.");
        }
    }
    async importFromFiles(arg1, ...files) {
        const dsId = await this.getInfo().then((info) => info.id);
        this._throwIfJobRunning(dsId);
        try {
            datasetsWithOngoingJob[dsId] = true;
            let baseIRI;
            let defaultGraphName;
            let overwriteAll;
            if (typeof arg1 === "string") {
                files = [arg1].concat(files);
            }
            else if ("name" in arg1) {
                files = [arg1].concat(files);
            }
            else {
                baseIRI = arg1.baseIRI;
                defaultGraphName = arg1.defaultGraphName;
                overwriteAll = arg1.overwriteAll;
            }
            if (overwriteAll && !(await this._app.isCompatible("2.2.7"))) {
                throw new Error_1.IncompatibleError("Overwriting graphs is only supported by TriplyDB API version 2.2.7 or greater");
            }
            const job = new JobUpload({
                app: this._app,
                baseIRI: baseIRI,
                defaultGraphName: defaultGraphName,
                overwriteAll: !!overwriteAll,
                datasetPath: await this._getDatasetPath(),
                datasetNameWithOwner: await this._getDatasetNameWithOwner(),
            });
            this.lastJob = await job.create();
            await this.lastJob.uploadFiles(...files);
            await this.lastJob.exec();
            await this.getInfo(true);
            return this;
        }
        finally {
            delete datasetsWithOngoingJob[dsId];
        }
    }
    async importFromStore(optsOrStore, _store) {
        let store;
        let opts = {};
        if (_store instanceof n3.Store) {
            store = _store;
            opts = optsOrStore;
        }
        else {
            store = optsOrStore;
        }
        const quads = store.getQuads(null, null, null, null);
        const quadsString = new n3.Writer().quadsToString(quads);
        const tmpFile = path_1.default.resolve(os_1.tmpdir(), `triplydb-${md5_1.default(quadsString)}.trig`);
        await fs.writeFile(tmpFile, quadsString, "utf-8");
        return this.importFromFiles(opts || {}, tmpFile);
    }
    async importFromUrls(arg1, ...urls) {
        const dsId = await this.getInfo().then((info) => info.id);
        try {
            this._throwIfJobRunning(dsId);
            datasetsWithOngoingJob[dsId] = true;
            let baseIRI;
            let defaultGraphName;
            let overwriteAll;
            if (typeof arg1 === "string") {
                urls = [arg1].concat(urls);
            }
            else {
                baseIRI = arg1.baseIRI;
                defaultGraphName = arg1.defaultGraphName;
                overwriteAll = arg1.overwriteAll;
            }
            if (overwriteAll && !(await this._app.isCompatible("2.2.7"))) {
                throw new Error_1.IncompatibleError("Overwriting graphs is only supported by TriplyDB API version 2.2.7 or greater");
            }
            const ownerName = await this._owner.getName();
            let info = await RequestHandler_1._post({
                errorWithCleanerStack: Error_1.getErr(`Failed to delete import from ${urls.length} URLs in dataset ${await this._getDatasetNameWithOwner()}.`),
                app: this._app,
                path: "/datasets/" + ownerName + "/" + this._name + "/jobs",
                data: {
                    type: "download",
                    downloadUrls: urls,
                    defaultGraphName: defaultGraphName,
                    overwriteAll: !!overwriteAll,
                    baseIRI: baseIRI,
                },
            });
            const jobUrl = `${this._app["_config"].url}${await this._getDatasetPath("/jobs/" + info.jobId)}`;
            info = await waitForJobToFinish(this._app, jobUrl, (await this.getInfo()).id);
            await this.getInfo(true);
            return this;
        }
        finally {
            delete datasetsWithOngoingJob[dsId];
        }
    }
    async describe(iri) {
        const iriString = typeof iri === "string" ? iri : iri.value;
        const buffer = await RequestHandler_1._get({
            app: this._app,
            path: await this._getDatasetPath("/describe.nt"),
            query: {
                resource: iriString,
            },
            expectedResponseBody: "buffer",
            errorWithCleanerStack: Error_1.getErr(`Failed to describe '${iri}' of ${await this._getDatasetNameWithOwner()}.`),
        });
        return new n3.Parser().parse(buffer.toString());
    }
    getStatements(payload) {
        return new AsyncIteratorHelper_1.default({
            error: Error_1.getErr(`Failed to get statements`),
            getErrorMessage: async () => `Failed to get statements of dataset ${await this._getDatasetNameWithOwner()}.`,
            app: this._app,
            mapResult: async (info) => info,
            getUrl: async () => this._app["_config"].url +
                (await this._getDatasetPath("/statements")) +
                "?" +
                query_string_1.stringify(Object.assign({ limit: 50 }, lodash_1.pick(payload, "subject", "predicate", "object", "graph"))),
        });
    }
    async uploadAsset(fileOrPath, assetName) {
        if (!assetName) {
            if (typeof fileOrPath === "string") {
                assetName = fileOrPath;
            }
            else {
                assetName = fileOrPath.name;
            }
        }
        let assetAlreadyExists = false;
        try {
            await this.getAsset(assetName);
            assetAlreadyExists = true;
        }
        catch (e) {
            if (e instanceof Error_1.TriplyDbJsError && e.statusCode === 404) {
            }
            else {
                throw e;
            }
        }
        if (assetAlreadyExists) {
            throw Error(`Tried to add asset '${assetName}' to dataset ${await this._getDatasetNameWithOwner()}, but an asset with that name already exists.`);
        }
        return new Asset_1.default(this, await Asset_1.default["uploadAsset"]({ fileOrPath, assetName, dataset: this }));
    }
    async addService(type, name, reasoner) {
        return await new Service_1.default({
            app: this._app,
            datasetPath: await this._getDatasetPath(),
            datasetNameWithOwner: await this._getDatasetNameWithOwner(),
            name,
            type,
            reasoner,
        }).create();
    }
    async addDatasetPrefixes(newPrefixes) {
        const asPairs = lodash_1.toPairs(newPrefixes);
        await RequestHandler_1._patch({
            errorWithCleanerStack: Error_1.getErr(`Failed to add ${lodash_1.size(newPrefixes)} prefixes to dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: await this._getDatasetPath("/prefixes"),
            data: asPairs.map(([key, value]) => ({
                prefixLabel: key,
                iri: value,
                scope: "local",
            })),
        });
        return this.getPrefixes(true);
    }
    async removeDatasetPrefixes(prefixLabels) {
        const dsPath = await this._getDatasetPath();
        await Promise.all(prefixLabels.map(async (p) => RequestHandler_1._delete({
            errorWithCleanerStack: Error_1.getErr(`Failed to delete prefix ${p} from dataset ${await this._getDatasetNameWithOwner()}.`),
            app: this._app,
            path: dsPath + "/prefixes/" + p,
            expectedResponseBody: "empty",
        }).catch((e) => {
            if (e instanceof Error_1.TriplyDbJsError && e.statusCode === 404) {
                return;
            }
            throw e;
        })));
        return this.getPrefixes(true);
    }
    async getPrefixes(refresh = false) {
        if (refresh || !this.allPrefixes) {
            const prefixes = await RequestHandler_1._get({
                errorWithCleanerStack: Error_1.getErr(`Failed to get prefixes of dataset ${await this._getDatasetNameWithOwner()}.`),
                app: this._app,
                path: await this._getDatasetPath("/prefixes"),
            });
            this.allPrefixes = lodash_1.fromPairs(prefixes.map((p) => [p.prefixLabel, p.iri]));
        }
        return this.allPrefixes;
    }
}
exports.default = Dataset;
const datasetsWithOngoingJob = {};
async function waitForJobToFinish(app, jobUrl, dsId) {
    let waitFor = 100;
    const check = async () => {
        var _a;
        const info = await RequestHandler_1._get({
            errorWithCleanerStack: Error_1.getErr(`Failed to get upload job status`).addContext({ jobUrl }),
            app: app,
            url: jobUrl,
        });
        if (info.status === "error")
            throw Error_1.getErr(((_a = info.error) === null || _a === void 0 ? void 0 : _a.message) || "Failed to upload file");
        if (info.status === "canceled" || info.status === "finished")
            return info;
        await utils_1.wait(waitFor);
        if (waitFor < 10 * 1000)
            waitFor = waitFor * 2;
        return check();
    };
    try {
        return check();
    }
    finally {
        delete datasetsWithOngoingJob[dsId];
    }
}
const maxJobUploadWindow = 8;
class JobUpload {
    constructor(conf) {
        this._config = conf;
    }
    getJobUrl() {
        return this.jobUrl;
    }
    async create() {
        this._info = await RequestHandler_1._post({
            errorWithCleanerStack: Error_1.getErr(`Failed to create job for dataset ${this._config.datasetNameWithOwner}.`),
            app: this._config.app,
            path: this._config.datasetPath + "/jobs",
            data: {
                defaultGraphName: this._config.defaultGraphName,
                baseIRI: this._config.baseIRI,
            },
        });
        this.jobUrl = `${this._config.app["_config"].url}${this._config.datasetPath}/jobs/${this._info.jobId}`;
        return this;
    }
    info() {
        return this._info;
    }
    async uploadFile(fileOrPath) {
        let rs;
        let fileSize;
        let fileName;
        if (typeof fileOrPath === "string") {
            if (fs.createReadStream === undefined) {
                throw Error_1.getErr('"fs" is not loaded in this environment, use a "File" instead');
            }
            rs = fs.createReadStream(fileOrPath);
            fileSize = (await fs.stat(fileOrPath)).size;
            fileName = fileOrPath;
        }
        else {
            rs = fileOrPath;
            fileSize = fileOrPath.size;
            fileName = fileOrPath.name;
        }
        return new Promise((resolve, reject) => {
            const upload = new tus.Upload(rs, {
                endpoint: this.jobUrl + "/add",
                resume: true,
                metadata: {
                    filename: fileName,
                },
                headers: {
                    Authorization: "Bearer " + this._config.app["_config"].token,
                },
                chunkSize: 5 * 1024 * 1024,
                uploadSize: fileSize,
                retryDelays: [2000, 5000, 10000, 40000, 50000],
                onError: function (error) {
                    reject(error);
                },
                onProgress: function (_bytesUploaded, _bytesTotal) { },
                onSuccess: function () {
                    log("finished file " + fileOrPath);
                    resolve();
                },
            });
            upload.start();
        });
    }
    async uploadFiles(...files) {
        const promises = [];
        const getFromStack = async () => {
            const file = files.pop();
            if (!file)
                return;
            await this.uploadFile(file);
            return getFromStack();
        };
        for (let i = 0; i < Math.min(maxJobUploadWindow, files.length); i++) {
            promises.push(getFromStack());
        }
        await Promise.all(promises);
        await this.refresh();
        return this;
    }
    async refresh() {
        if (!this.jobUrl)
            throw Error_1.getErr("Cannot refresh uninstantiated job");
        this._info = await RequestHandler_1._get({
            errorWithCleanerStack: Error_1.getErr(`Failed to get job information for dataset ${this._config.datasetNameWithOwner}.`).addContext({
                jobUrl: this.jobUrl,
            }),
            url: this.jobUrl,
            app: this._config.app,
        });
    }
    async exec() {
        if (!this.jobUrl)
            throw Error_1.getErr("Cannot start uninstantiated job");
        this._info = await RequestHandler_1._post({
            errorWithCleanerStack: Error_1.getErr(`Failed to start job in dataset ${this._config.datasetNameWithOwner}.`).addContext({
                jobUrl: this.jobUrl,
            }),
            app: this._config.app,
            url: this.jobUrl + "/start",
            data: {
                overwriteAll: !!this._config.overwriteAll,
            },
        });
        this._info = await waitForJobToFinish(this._config.app, this.jobUrl, this._info.datasetId);
    }
}
exports.JobUpload = JobUpload;
//# sourceMappingURL=Dataset.js.map